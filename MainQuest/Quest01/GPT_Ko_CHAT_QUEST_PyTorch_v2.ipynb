{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb00bab",
   "metadata": {},
   "source": [
    "\n",
    "# **Korean GPT (Decoder-only) Pretraining** *(PyTorch)*\n",
    "\n",
    "\n",
    "\n",
    "## [1] Transformer와 비교해 변경이 필요한 부분\n",
    "- (기존) Encoder–Decoder → (본 과제) **Decoder-only GPT**  \n",
    "  - **Cross-Attention 제거**, **Causal Self-Attention**(미래 토큰 마스킹) 적용  ← `# [CHG]` 주석으로 코드에 표시  \n",
    "  - 목적 함수: `L1 = ∑ log P(u_i | u_<i)` (다음 토큰 예측)  \n",
    "- 입력/출력: **BOS** + 토큰[0..T-2] → 라벨: 토큰[1..T-1] + **EOS**  \n",
    "- 위치 정보: **학습형 Positional Embedding** 추가 (GPT-1 스타일)  \n",
    "- 헤드: **LM Head with Weight Tying** (임베딩 가중치 공유)  \n",
    "- 학습: **AdamW + Cosine LR with Warmup + AMP + Gradient Clipping + Early Stopping**\n",
    "\n",
    "---\n",
    "\n",
    "## [2] 모델 입력 형태에 맞게 전처리 수행\n",
    "- **SentencePiece(Unigram)** 사용 (첨부 노트북 참조).  \n",
    "  - 특수 토큰 예약: `[PAD]=0, [BOS]=1, [EOS]=2, [UNK]=3, [USR]=4, [SYS]=5`  \n",
    "  - 대화 데이터는 `<usr> ... </s> <sys> ... </s>` 형식으로 하나의 시퀀스로 결합 → **사전학습(언어모델)**에 바로 사용.\n",
    "- 오프라인 환경 또는 라이브러리 미설치 시 **Fallback 토크나이저**(공백/문자 수준) 제공.\n",
    "\n",
    "---\n",
    "\n",
    "## [3] 입력 블록을 GPT 논문에 기반하여 수정\n",
    "- 입력 텐서 = **TokenEmbedding + PositionalEmbedding(learned)**  \n",
    "- 샘플 배치를 프린트하여 **입력 정상 확인** + **길이/마스크 검증**\n",
    "\n",
    "---\n",
    "\n",
    "## [4] GPT 모델을 정상적으로 구성 (요약/학습 로그 첨부)\n",
    "- PyTorch **Decoder-only Transformer** 구현 (Causal mask)  \n",
    "- `print(#params)`, 학습 로그(loss/perplexity), 체크포인트 저장\n",
    "\n",
    "---\n",
    "\n",
    "## [5] 입력에 따른 출력 생성\n",
    "- **Top-k / Top-p / Temperature / Repetition Penalty** 지원  \n",
    "- 프롬프트에 대한 **생성 예시 출력**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4d28244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports & Config\n",
    "import os, re, math, random, time, json, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "try:\n",
    "    import sentencepiece as spm\n",
    "    HAS_SPM = True\n",
    "except Exception:\n",
    "    HAS_SPM = False\n",
    "\n",
    "SEED=42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "SEQ_LEN=128; BATCH=64; EPOCHS=5\n",
    "EMB_DIM=512; NUM_HEADS=8; FF_DIM=EMB_DIM*4; NUM_LAYERS=8; DROPOUT=0.1\n",
    "LR=3e-4; WARMUP_STEPS=200; WEIGHT_DECAY=0.1; GRAD_CLIP=1.0; PATIENCE=3\n",
    "NGRAM_BLOCK=0  # 0=off, 2/3 for no-repeat n-gram\n",
    "\n",
    "SPECIAL_TOKENS={\"PAD\":0,\"BOS\":1,\"EOS\":2,\"UNK\":3}\n",
    "DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f330494c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ChatbotData.csv: 11823\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data: build <usr>/<sys>/<eot> sequences\n",
    "DATA_CSV=\"data/ChatbotData.csv\"\n",
    "def build_dialog_rows_from_csv(path):\n",
    "    df=pd.read_csv(path)[[\"Q\",\"A\"]].dropna()\n",
    "    rows=[]\n",
    "    for q,a in zip(df[\"Q\"], df[\"A\"]):\n",
    "        q=str(q).strip(); a=str(a).strip()\n",
    "        if not q or not a: continue\n",
    "        rows.append(f\"<usr> {q} <eot> <sys> {a} <eot>\")\n",
    "    return rows\n",
    "\n",
    "if os.path.exists(DATA_CSV):\n",
    "    rows=build_dialog_rows_from_csv(DATA_CSV)\n",
    "    text_corpus=\"\\n\".join(rows)\n",
    "    print(\"Loaded ChatbotData.csv:\", len(rows))\n",
    "else:\n",
    "    text_corpus=(\n",
    "        \"<usr> 한국어 GPT 모델을 만들고 싶어 <eot> <sys> 사전학습 후 미세조정을 하면 좋아요 <eot>\\n\"\n",
    "        \"<usr> 어떤 토크나이저가 좋아? <eot> <sys> sentencepiece 권장! 없으면 공백 기반도 가능 <eot>\\n\"\n",
    "        \"<usr> 학습을 안정적으로 하려면? <eot> <sys> 워밍업, 코사인 스케줄, AMP, weight decay, 반복 패널티 <eot>\"\n",
    "    )\n",
    "    print(\"Using tiny demo corpus.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5adda396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 21807\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenizer (SentencePiece -> fallback)\n",
    "MODEL_PREFIX=\"spm_ko_chat\"; VOCAB_SIZE=16000\n",
    "if HAS_SPM:\n",
    "    if not os.path.exists(MODEL_PREFIX+\".model\"):\n",
    "        with open(\"temp_corpus.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "            f.write(text_corpus)\n",
    "        spm.SentencePieceTrainer.Train(\n",
    "            input=\"temp_corpus.txt\",\n",
    "            model_prefix=MODEL_PREFIX,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            model_type=\"unigram\",\n",
    "            pad_id=SPECIAL_TOKENS[\"PAD\"],\n",
    "            unk_id=SPECIAL_TOKENS[\"UNK\"],\n",
    "            bos_id=SPECIAL_TOKENS[\"BOS\"],\n",
    "            eos_id=SPECIAL_TOKENS[\"EOS\"],\n",
    "            user_defined_symbols=[\"<usr>\",\"<sys>\",\"<eot>\"],\n",
    "            character_coverage=0.9995\n",
    "        )\n",
    "    sp = spm.SentencePieceProcessor(model_file=MODEL_PREFIX+\".model\")\n",
    "    PAD_ID=sp.pad_id(); BOS_ID=sp.bos_id(); EOS_ID=sp.eos_id(); UNK_ID=sp.unk_id()\n",
    "    USR_ID=sp.piece_to_id(\"<usr>\"); SYS_ID=sp.piece_to_id(\"<sys>\"); EOT_ID=sp.piece_to_id(\"<eot>\")\n",
    "    VOCAB_SIZE=sp.vocab_size()\n",
    "else:\n",
    "    def basic_tokenize(t): return re.sub(r\"\\s+\",\" \",t.strip()).split(\" \")\n",
    "    vocab=[\"[PAD]\",\"[BOS]\",\"[EOS]\",\"[UNK]\",\"<usr>\",\"<sys>\",\"<eot>\"]\n",
    "    for tok in basic_tokenize(text_corpus):\n",
    "        if tok not in vocab: vocab.append(tok)\n",
    "    word2id={w:i for i,w in enumerate(vocab)}; id2word={i:w for w,i in word2id.items()}\n",
    "    PAD_ID=0; BOS_ID=1; EOS_ID=2; UNK_ID=3; USR_ID=word2id[\"<usr>\"]; SYS_ID=word2id[\"<sys>\"]; EOT_ID=word2id[\"<eot>\"]\n",
    "    VOCAB_SIZE=len(vocab)\n",
    "\n",
    "def encode(text):\n",
    "    if HAS_SPM: return sp.encode(text, out_type=int)\n",
    "    else: return [word2id.get(t, UNK_ID) for t in basic_tokenize(text)]\n",
    "\n",
    "def decode(ids):\n",
    "    if HAS_SPM: return sp.decode(ids)\n",
    "    else: return \" \".join([id2word.get(int(i), \"[UNK]\") for i in ids])\n",
    "\n",
    "print(\"Vocab size:\", VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77bb6684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: torch.Size([2083, 128]) torch.Size([2083, 128])\n",
      "X[0]->text: <usr> 오늘 마지막 인사하러 갑니다. <eot> <sys> 미련없이 정리했긴 바랍니다. <eot> <usr> 오늘 메일을 한통 보냈어 <eot> <sys> 후회가 남지 않길 바랍니다. <eot> <usr> 오늘 무너졌어. <eot> <sys> 힘내지 않아도 돼요. 조금 쉬어가세요. <eot> <usr> 오늘 무작정 찾으러가고싶은데 갈까? <eot> <sys> 전 추천하고 싶지 않아요. <eot> <usr> 오늘 보기로 했던 그녀, 못 보았네 <eot> <sys> 보지 않는 게 더 나았을 수도 있겠네요. <eot> <usr> 오늘 얼굴보고 확실히 헤어졌어 <eot> <sys> 쉽지 않을 결정이었을텐데 맘고생 많았어요. <eot> <usr> 오늘 연락왔네 그래도\n",
      "Y[0]->text: 오늘 마지막 인사하러 갑니다. <eot> <sys> 미련없이 정리했긴 바랍니다. <eot> <usr> 오늘 메일을 한통 보냈어 <eot> <sys> 후회가 남지 않길 바랍니다. <eot> <usr> 오늘 무너졌어. <eot> <sys> 힘내지 않아도 돼요. 조금 쉬어가세요. <eot> <usr> 오늘 무작정 찾으러가고싶은데 갈까? <eot> <sys> 전 추천하고 싶지 않아요. <eot> <usr> 오늘 보기로 했던 그녀, 못 보았네 <eot> <sys> 보지 않는 게 더 나았을 수도 있겠네요. <eot> <usr> 오늘 얼굴보고 확실히 헤어졌어 <eot> <sys> 쉽지 않을 결정이었을텐데 맘고생 많았어요. <eot> <usr> 오늘 연락왔네 그래도 이젠\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build sequences (X,Y)\n",
    "def build_sequences(corpus_text, seq_len=SEQ_LEN, stride=SEQ_LEN//2):\n",
    "    ids=[BOS_ID]+encode(corpus_text)+[EOS_ID]\n",
    "    X,Y=[],[]\n",
    "    for start in range(0, max(1,len(ids)-1-seq_len), stride):\n",
    "        x=ids[start:start+seq_len]; y=ids[start+1:start+1+seq_len]\n",
    "        if len(x)<seq_len or len(y)<seq_len: break\n",
    "        X.append(x); Y.append(y)\n",
    "    import torch\n",
    "    return torch.tensor(X,dtype=torch.long), torch.tensor(Y,dtype=torch.long)\n",
    "\n",
    "X,Y=build_sequences(text_corpus, SEQ_LEN)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self,X,Y): self.X=X; self.Y=Y\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self,idx): return self.X[idx], self.Y[idx]\n",
    "train_loader=DataLoader(LMDataset(X,Y), batch_size=BATCH, shuffle=True, drop_last=True)\n",
    "\n",
    "xb,yb=next(iter(train_loader))\n",
    "print(\"Dataset:\", X.shape, Y.shape)\n",
    "print(\"X[0]->text:\", decode(xb[0].tolist()[:80]))\n",
    "print(\"Y[0]->text:\", decode(yb[0].tolist()[:80]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34e26f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (emb): TokenPositionalEmbedding(\n",
      "    (token_emb): Embedding(21807, 512)\n",
      "    (pos_emb): Embedding(128, 512)\n",
      "  )\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-7): 8 x GPTBlock(\n",
      "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (drop1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (token_emb): Embedding(21807, 512)\n",
      ")\n",
      "Total params: 36450816\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model — Decoder-only GPT\n",
    "class TokenPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.token_emb=nn.Embedding(vocab_size, emb_dim)\n",
    "        self.pos_emb=nn.Embedding(max_len, emb_dim)\n",
    "    def forward(self,x):\n",
    "        B,T=x.shape\n",
    "        pos=torch.arange(0,T, device=x.device).unsqueeze(0)\n",
    "        return self.token_emb(x)+self.pos_emb(pos)  # [CHG] 위치 정보 추가\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1=nn.LayerNorm(emb_dim)\n",
    "        self.attn=nn.MultiheadAttention(embed_dim=emb_dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.drop1=nn.Dropout(dropout)\n",
    "        self.ln2=nn.LayerNorm(emb_dim)\n",
    "        self.ff=nn.Sequential(nn.Linear(emb_dim,ff_dim), nn.GELU(), nn.Linear(ff_dim,emb_dim), nn.Dropout(dropout))\n",
    "    def forward(self,x,attn_mask=None):\n",
    "        h=self.ln1(x)\n",
    "        if attn_mask is not None: attn_mask=attn_mask.to(x.device)  # [CHG]\n",
    "        attn_out,_=self.attn(h,h,h, attn_mask=attn_mask)            # [CHG] Causal Self-Attention\n",
    "        x=x+self.drop1(attn_out)\n",
    "        x=x+self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, emb_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb=TokenPositionalEmbedding(vocab_size, emb_dim, seq_len)\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        self.blocks=nn.ModuleList([GPTBlock(emb_dim,num_heads,ff_dim,dropout) for _ in range(num_layers)])\n",
    "        self.ln_f=nn.LayerNorm(emb_dim)\n",
    "        self.token_emb=self.emb.token_emb\n",
    "        mask=torch.triu(torch.ones(seq_len,seq_len), diagonal=1)\n",
    "        mask=mask.masked_fill(mask==1, float(\"-inf\"))\n",
    "        self.register_buffer(\"causal_mask\", mask)\n",
    "    def forward(self,x):\n",
    "        h=self.drop(self.emb(x))\n",
    "        for blk in self.blocks:\n",
    "            h=blk(h, attn_mask=self.causal_mask)\n",
    "        h=self.ln_f(h)\n",
    "        logits=torch.matmul(h, self.token_emb.weight.T)  # [CHG] LM head (tied)\n",
    "        return logits\n",
    "\n",
    "def count_params(m): return sum(p.numel() for p in m.parameters())\n",
    "\n",
    "model=GPTModel(VOCAB_SIZE, SEQ_LEN, EMB_DIM, NUM_HEADS, FF_DIM, NUM_LAYERS, DROPOUT).to(DEVICE)\n",
    "print(model)\n",
    "print(\"Total params:\", count_params(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7e49434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_307/1407533660.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler=torch.cuda.amp.GradScaler(enabled=(DEVICE=='cuda'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optimizer/Scheduler/AMP/Early stopping\n",
    "from torch.optim import AdamW\n",
    "optimizer=AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scaler=torch.cuda.amp.GradScaler(enabled=(DEVICE=='cuda'))\n",
    "\n",
    "def cosine_schedule(step, warmup, total, base_lr):\n",
    "    if step < warmup: return base_lr*(step+1)/warmup\n",
    "    progress=(step-warmup)/max(1,total-warmup)\n",
    "    return 0.5*base_lr*(1+math.cos(math.pi*progress))\n",
    "\n",
    "def set_lr(opt, lr):\n",
    "    for pg in opt.param_groups: pg['lr']=lr\n",
    "\n",
    "total_steps=EPOCHS*len(train_loader)\n",
    "best_val=float('inf'); patience=PATIENCE; global_step=0\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_ppl(loader):\n",
    "    model.eval()\n",
    "    total_loss=0.0; total_tok=0\n",
    "    for xb,yb in loader:\n",
    "        xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
    "        logits=model(xb); B,T,V=logits.shape\n",
    "        loss=F.cross_entropy(logits.view(B*T,V), yb.view(B*T))\n",
    "        total_loss+=loss.item()*(B*T); total_tok+=B*T\n",
    "    avg=total_loss/max(1,total_tok)\n",
    "    ppl=math.exp(min(20,avg))\n",
    "    return avg,ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f3275a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_307/2228390807.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE=='cuda')):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train loss/token=218.3830 (ppl=485165195.41) | val loss=72.8744 (ppl=485165195.41) | 17.7s\n",
      "  ↳ Saved checkpoint: best_gpt.pt\n",
      "[Epoch 2] train loss/token=47.1861 (ppl=485165195.41) | val loss=34.8932 (ppl=485165195.41) | 13.5s\n",
      "  ↳ Saved checkpoint: best_gpt.pt\n",
      "[Epoch 3] train loss/token=33.1291 (ppl=485165195.41) | val loss=28.8556 (ppl=485165195.41) | 13.1s\n",
      "  ↳ Saved checkpoint: best_gpt.pt\n",
      "[Epoch 4] train loss/token=28.2932 (ppl=485165195.41) | val loss=26.2836 (ppl=485165195.41) | 13.0s\n",
      "  ↳ Saved checkpoint: best_gpt.pt\n",
      "[Epoch 5] train loss/token=25.0784 (ppl=485165195.41) | val loss=22.4719 (ppl=485165195.41) | 13.1s\n",
      "  ↳ Saved checkpoint: best_gpt.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_loss=0.0; epoch_tok=0; t0=time.time()\n",
    "    for xb,yb in train_loader:\n",
    "        xb=xb.to(DEVICE); yb=yb.to(DEVICE)\n",
    "        lr_now=cosine_schedule(global_step, WARMUP_STEPS, total_steps, LR)\n",
    "        set_lr(optimizer, lr_now)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE=='cuda')):\n",
    "            logits=model(xb); B,T,V=logits.shape\n",
    "            loss=F.cross_entropy(logits.view(B*T,V), yb.view(B*T))\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer); scaler.update()\n",
    "        epoch_loss+=loss.item()*(B*T); epoch_tok+=B*T; global_step+=1\n",
    "    avg=epoch_loss/max(1,epoch_tok); ppl=math.exp(min(20,avg))\n",
    "    val_loss,val_ppl=evaluate_ppl(train_loader)\n",
    "    print(f\"[Epoch {epoch}] train loss/token={avg:.4f} (ppl={ppl:.2f}) | val loss={val_loss:.4f} (ppl={val_ppl:.2f}) | {time.time()-t0:.1f}s\")\n",
    "    if val_loss < best_val-1e-4:\n",
    "        best_val=val_loss; patience=PATIENCE\n",
    "        torch.save(model.state_dict(), \"best_gpt.pt\"); print(\"  ↳ Saved checkpoint: best_gpt.pt\")\n",
    "    else:\n",
    "        patience-=1\n",
    "        if patience==0:\n",
    "            print(\"Early stopping triggered.\"); break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b19c8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generation Demo (system-only) ===\n",
      "만나보세요. 만나보세요. 돼라 돼라 너무 마세요. 마세요.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generation — return only the <sys> turn\n",
    "@torch.no_grad()\n",
    "def generate(user_text, max_new_tokens=80, temperature=0.8, top_k=50, top_p=0.9, rep_penalty=1.15):\n",
    "    model.eval()\n",
    "    prompt=f\"<usr> {user_text.strip()} <eot> <sys>\"\n",
    "    ids=[BOS_ID]+encode(prompt)\n",
    "    ids=ids[-SEQ_LEN:]\n",
    "    x=torch.zeros((1,SEQ_LEN),dtype=torch.long, device=DEVICE)\n",
    "    cur=len(ids); x[0,:cur]=torch.tensor(ids, device=DEVICE)\n",
    "\n",
    "    recent=[]\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits=model(x)[0,cur-1,:]\n",
    "        for t in recent[-32:]: logits[t]/=rep_penalty\n",
    "        # 금지 토큰(시스템 턴에서 <usr>/<sys> 출력 억제)\n",
    "        logits[USR_ID]=-1e10; logits[SYS_ID]=-1e10\n",
    "        logits=logits/max(1e-6,temperature)\n",
    "        probs=F.softmax(logits,dim=-1)\n",
    "        if top_k>0:\n",
    "            topv,topi=torch.topk(probs, k=min(top_k, probs.numel()))\n",
    "            mask=torch.zeros_like(probs); mask[topi]=probs[topi]; probs=mask\n",
    "        if top_p<1.0:\n",
    "            sort_probs,sort_idx=torch.sort(probs, descending=True)\n",
    "            cumsum=torch.cumsum(sort_probs,dim=-1)\n",
    "            cutoff=(cumsum>top_p).nonzero(as_tuple=False)\n",
    "            if cutoff.numel()>0:\n",
    "                k=cutoff[0,0].item()\n",
    "                keep=sort_idx[:k+1]; mask=torch.zeros_like(probs); mask[keep]=probs[keep]; probs=mask\n",
    "        probs=probs/probs.sum()\n",
    "        next_id=torch.multinomial(probs,1).item()\n",
    "        recent.append(next_id)\n",
    "        if cur>=SEQ_LEN: x=torch.roll(x,shifts=-1,dims=1); x[0,-1]=next_id\n",
    "        else: x[0,cur]=next_id; cur+=1\n",
    "        if next_id in {EOS_ID, EOT_ID}: break\n",
    "\n",
    "    # <sys> 구간만 추출\n",
    "    out_ids=x[0,:cur].tolist()\n",
    "    try: sys_start=out_ids.index(SYS_ID)+1\n",
    "    except ValueError: sys_start=0\n",
    "    try: sys_end=out_ids.index(EOT_ID, sys_start)\n",
    "    except ValueError: sys_end=cur\n",
    "    return decode(out_ids[sys_start:sys_end])\n",
    "\n",
    "print(\"=== Generation Demo (system-only) ===\")\n",
    "print(generate(\"안녕! 오늘 기분 어때?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752c33a-4e06-4202-bc1b-c4c714bbba0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
